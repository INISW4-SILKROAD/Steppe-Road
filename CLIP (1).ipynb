{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git C:\\Users\\hyeju\\AppData\\Local\\Temp\\pip-req-build-6o7hc1_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import clip\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load CLIP model and preprocessing\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# Dataset class definition\n",
    "class FabricDataset(Dataset):\n",
    "    def __init__(self, images, labels, ratios):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.ratios = ratios\n",
    "        self.bilinear = nn.Bilinear(512, len(ratios[0]), 512)  # Bilinear layer for combining image and ratio vectors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = preprocess(self.images[idx]).to(device)\n",
    "        with torch.no_grad():\n",
    "            img_features = model.encode_image(img.unsqueeze(0)).float()\n",
    "        img_features /= img_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        ratios_tensor = torch.tensor(self.ratios[idx], dtype=torch.float32).to(device)\n",
    "        combined_features = self.bilinear(img_features, ratios_tensor.unsqueeze(0))\n",
    "\n",
    "        return combined_features.squeeze(0), self.labels[idx]\n",
    "\n",
    "# Function to load images\n",
    "def load_image(image_path):\n",
    "    return Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Function to create datasets and dataloaders\n",
    "def create_dataset_and_dataloader(images, labels, ratios, batch_size=4):\n",
    "    dataset = FabricDataset(images, labels, ratios)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataset, dataloader\n",
    "\n",
    "# Function to save embeddings and labels\n",
    "def save_embeddings_and_labels(dataset, filename):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    for features, label in DataLoader(dataset, batch_size=1):\n",
    "        embeddings.append(features.squeeze(0).detach().cpu().numpy())\n",
    "        labels.append(label.item())\n",
    "    embeddings = np.array(embeddings)\n",
    "    labels = np.array(labels)\n",
    "    np.savez(filename, embeddings=embeddings, labels=labels)\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv('C:/Users/hyeju/Downloads/one_multi/hahaha/CLIP.csv')\n",
    "\n",
    "# Absolute path to the directory containing the images\n",
    "image_dir = r'C:/Users/hyeju/Downloads/one_multi/hahaha/GGMU'\n",
    "data['path'] = data['path'].apply(lambda x: os.path.join(image_dir, os.path.basename(x)))\n",
    "\n",
    "# Load and preprocess all images\n",
    "images = [load_image(img_path) for img_path in data['path']]\n",
    "\n",
    "# Prepare ratios by including all 13 material columns\n",
    "material_columns = ['cotton', 'polyester', 'acril', 'nylon', 'rayon', 'span', 'linen', 'polyurethane', 'modal', 'wool', 'tencel', 'acetate']\n",
    "ratios = data[material_columns].values.tolist()\n",
    "\n",
    "# Extract the target properties and convert them to integer labels\n",
    "properties = ['softness', 'lubricity', 'thickness', 'elasticity']\n",
    "labels_dict = {prop: data[prop].values for prop in properties}\n",
    "\n",
    "# Create datasets, dataloaders and save embeddings for each property\n",
    "for prop in properties:\n",
    "    dataset, dataloader = create_dataset_and_dataloader(images, labels_dict[prop], ratios)\n",
    "    save_embeddings_and_labels(dataset, f'C:/Users/hyeju/Downloads/one_multi/{prop}_embeddings_org_bi.npz')\n",
    "\n",
    "print(\"Embeddings and labels saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load embeddings and labels from npz file\n",
    "def load_embeddings_and_labels(filename):\n",
    "    data = np.load(filename)\n",
    "    embeddings = data['embeddings']\n",
    "    labels = data['labels']\n",
    "    return embeddings, labels\n",
    "\n",
    "# Load and print the dimensions of embeddings and labels for each npz file\n",
    "softness_embeddings, softness_labels = load_embeddings_and_labels('C:/Users/hyeju/Downloads/one_multi/softness_embeddings_org_bi.npz')\n",
    "\n",
    "print(\"Softness embeddings shape:\", softness_embeddings.shape)\n",
    "print(\"Softness labels shape:\", softness_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .npz 파일 로드\n",
    "npz_file = np.load('C:/Users/hyeju/Downloads/one_multi/lubricity_embeddings_org_add.npz')\n",
    "\n",
    "# 파일에 저장된 배열의 키 목록 출력\n",
    "keys = list(npz_file.keys())\n",
    "print(\"Keys in the .npz file:\", keys)\n",
    "\n",
    "# 각 키에 대한 배열 출력 (생략 없이)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "for key in keys:\n",
    "    print(f\"Array for key '{key}':\")\n",
    "    print(npz_file[key])\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagebind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
